{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "1. Model:",
   "id": "4b647ea7a5b2a411"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as functional\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "\n",
    "        # Calculate the size of the output from conv2 after flattening\n",
    "        self.fc_input_size = self._get_conv_output_size((3, 224, 224))\n",
    "\n",
    "        self.fc1 = nn.Linear(self.fc_input_size, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def _get_conv_output_size(self, shape):\n",
    "        # Create a dummy tensor with the shape of an input image\n",
    "        dummy_input = torch.zeros(1, *shape)\n",
    "        # Forward pass through conv1, pool, conv2, and pool\n",
    "        dummy_output = self.pool(functional.relu(self.conv1(dummy_input)))\n",
    "        dummy_output = self.pool(functional.relu(self.conv2(dummy_output)))\n",
    "        # Flatten the output tensor\n",
    "        return int(torch.flatten(dummy_output, 1).size(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(functional.relu(self.conv1(x)))\n",
    "        x = self.pool(functional.relu(self.conv2(x)))\n",
    "\n",
    "        # flatten all dimensions except batch\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        x = functional.relu(self.fc1(x))\n",
    "        x = functional.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Define transforms for the training and testing sets\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to a fixed size\n",
    "    transforms.ToTensor()  # Convert images to tensor\n",
    "])"
   ],
   "id": "4ce3882724aca408"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Full Dataset:\n",
    "\n",
    "* Warhol -> 182\n",
    "* DaVinci -> 205\n",
    "* Monet -> 498\n",
    "* Renoir -> 498\n",
    "* Gogh -> 650\n",
    "* Rembrandt -> 655\n",
    "* Degas -> 703\n",
    "* Dali -> 740\n",
    "* Picasso -> 992\n",
    "* Munch -> 1765\n",
    "\n",
    "Accuracy: 50%, every picture is predicted as \"Munch\""
   ],
   "id": "7cb49fec559a9baf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Created class data_selector to limit the amount of images the model is training on. \n",
    "Images limited to 180 since only 182 Warhol images were found\n",
    "\n",
    "Accuracy dropped to 13%"
   ],
   "id": "ab1a78413d3be219"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Increased number of epochs from 2 to 4, batches from 32 to 10, accuracy to 22%",
   "id": "7b07bd93953b4da8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Increased image size to 2000 x 2000, runtime error\n",
    "\n",
    "Fixed bug, image size decreased to 512x512\n",
    "\n",
    "Removed .jfif files, which are not recognised by torch\n",
    "\n",
    "Increased Epochs to 20, loss reached low levels, accuracy still just 36%"
   ],
   "id": "99dc44a9887d018d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Decreased Epochs to 10, loss decreased but not that low, accuracy dropped to 20%",
   "id": "c8e23c863c110fdd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![](JupiterPictures/LossPlot512x512.png)",
   "id": "535a6790a68731ca"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Decreased image size to 50x50, reached 34% accuracy",
   "id": "323cf7487fe2ba3f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Decreased image size to 32x32, increased batch size to 20, epochs to 40, reached 29% accuracy",
   "id": "4cec7e982de1ea8a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![](JupiterPictures/LossPlot32x32.png)",
   "id": "e74f17402159b499"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Reverted batch size to 5",
   "id": "82ff85521f9b32ac"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "slighly different data set (different vanGogh i think), changed training set to 250 instead of 180. \n",
    "32% accuracy, munch random the rest either ~40% or ~20% accuracy"
   ],
   "id": "fea1bc0aec4bb1a1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "![](JupiterPictures/lossPlot250TrainingSize.png)\n",
    "\n"
   ],
   "id": "3008a85303b5992b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Alteration: \n",
    "    raised trianing data size to 350, warhol davinci only classes without enough data\n",
    "Results:\n",
    "    33% accuacy\n",
    "    gogh random\n",
    "    rest evenly range from 20-55%"
   ],
   "id": "d28964431fea19b9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![](JupiterPictures/LossPlot350TrainingSize.png)\n",
   "id": "f229bd4ece88377f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Alteration; training size now 500. classes with data<500 Renoir Warhol Monet Davinci Gogh\n",
    "Results:\n",
    "32% accuracy\n",
    "Accuracy for class: Dali  is 24.5 %\n",
    "Accuracy for class: Degas is 43.0 %\n",
    "Accuracy for class: Gogh  is 19.0 %\n",
    "Accuracy for class: Monet is 30.2 %\n",
    "Accuracy for class: Munch is 37.6 %\n",
    "Accuracy for class: Picasso is 25.7 %\n",
    "Accuracy for class: Rembrandt is 55.6 %\n",
    "Accuracy for class: Renoir is 26.8 %\n",
    "Accuracy for class: Warhol is 14.3 %\n",
    "Accuracy for class: daVinci is 39.5 %"
   ],
   "id": "e15ed89dd86eced1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![](JupiterPictures/LossPlot500TrainingSize.png)\n",
   "id": "190c731db9f13e45"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Alteration: training size 400, 35 epochs.",
   "id": "219ff3dad5a522c2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Implemented data augmentation (horizontal + vertical flip) and best epoch selection. Best epoch alone delivered 44% accuracy, with data augmentation only 41%, although all classes have similar accuracies",
   "id": "f515ff91d4c48383"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Added various choices for parameters to find the real best model:\n",
    "\n",
    "batch_size_options = [5, 20, 40]\n",
    "\n",
    "learning_rate_options = [0.01, 0.001]\n",
    "\n",
    "Training was very slow and results with batch size 5 were similar or lower to batch size 10 (40.83% - 43.15% +), so 5 was removed\n",
    "\n",
    "Accuracy for best model on test data was disappointing, only 15%"
   ],
   "id": "ebc30d23d1e19607"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Reverted to single model training, reached 45% accuracy on test data with bach size 20, 10 epochs and learning rate 0.01",
   "id": "4b5bb1c55adbf525"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "f950ce1790fa8ac1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
